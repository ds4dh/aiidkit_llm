# Run configuration and task (data collator) definition
hf_data_dir: /home/shares/ds4dh/aiidkit_project/data_new/processed/v1/teav
result_dir: results_optuna/infection_bacteria  # base directory name for all results
data_collator:
  eav_mappings:
    entity_id: entity
    attribute_id: attribute
    value_id: value_binned
  time_mapping:
    days_since_tpx: time
  mlm_label_keys:
    - entity_id       # mlm task must predict 'entity_id' labels
    - attribute_id    # mlm task must predict 'attribute_id' labels
    - value_id        # mlm task must predict 'value_id' labels
  mlm_masking_rules:  # these numbers will set the result subdirectory name
    {"entity_id": 0.15, "attribute_id": 0.25, "value_id": 0.45}

# Configuration for finetuning tasks to run
train_data_augment: all  # "all", "none", "valid", which other fups are added to finetuning samples
target_undersampling_ratio: 10.0  # maximum ratio of positive to negative samples during finetuning
early_stopping_metric: roc_auc  # metric to use for early stopping (per label, then averaged)
prediction_tasks:  # horizon elements can be lists (multi-label) or integers / singletons (binary)
  infection_bacteria:   {fups: [0,  30,  60,  90,  180,  360], horizons: [[ 30,  60,  90,  180]]}
  # infection_virus:      {fups: [0,  30,  60,  90,  180,  360], horizons: [[ 30,  60,  90,  180]]}
  # graft_loss:           {fups: [0, 360, 720, 1080, 1800],     horizons: [[180, 360, 1080, 1800]]}
  # death:                {fups: [0, 360, 720, 1080, 1800],     horizons: [[180, 360, 1080, 1800]]}

# Arguments for optuna run (perform both pretraining and finetuning for different hyper-parameters)
optuna:
  metric_to_optimize: val_all_roc_auc  # if possible, will add prediction tasks common denominator
  num_trials: 50
  num_trials_debug: 2
  skip_pretraining_if_ckpt_exists: true
  db_name: optuna_study.db
  study_name: pretraining_finetuning_hp_optimization

# Arguments for HF's trainer for the pre-training phase
pretrainer:
  logging_steps: 10
  warmup_steps: 1000
  num_train_epochs: 100  # maximum number of epochs (loses to max_steps)
  max_steps: 100000  # maximum number of steps (training + validation)
  eval_strategy: steps
  save_strategy: steps
  eval_steps: 250
  save_steps: 250
  save_total_limit: 1
  gradient_accumulation_steps: 1
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  learning_rate: 1.0e-4  # careful! "1e-4" will be read as a string, use 1.0e-4
  weight_decay: 0.1
  bf16: true
  report_to: wandb
  remove_unused_columns: false  # crucial for custom collators
  load_best_model_at_end: true  # required for early stopping
  metric_for_best_model: eval_mlm_accuracy  # format: "eval_" + metric name
  greater_is_better: true
  early_stopping_patience: 5  # evaluations without improvement before stopping

# Arguments for HF's trainer for the fine-tuning phase
finetuner:
  logging_steps: 10
  warmup_steps: 1000
  num_train_epochs: 10  # maximum number of epochs (loses to max_steps)
  max_steps: 10000  # maximum number of steps (training + validation)
  eval_strategy: steps
  save_strategy: steps
  eval_steps: 100
  save_steps: 100
  save_total_limit: 1
  eval_accumulation_steps: 100
  gradient_accumulation_steps: 1
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  learning_rate: 4.0e-5
  weight_decay: 0.01
  loss_name: poly1  # ce, weighted_ce, focal, dice, poly1
  bf16: true
  report_to: wandb
  remove_unused_columns: false  # crucial for custom collators
  load_best_model_at_end: true
  metric_for_best_model: eval_all_early_stopping_metric  # see evaluator arguments
  greater_is_better: true
  early_stopping_patience: 10  # evaluations without improvement before stopping

# LoRA configuration for fine-tuning (experimentally, it did not improve results)
use_lora: false  # false for full fine-tuning
lora_config:
  r: 16
  lora_alpha: 32  # good guess: twice the rank
  lora_dropout: 0.1
  bias: "none"
  task_type: SEQ_CLS
  target_modules: all-linear

# Embedding model
model:
  model_id: answerdotai/ModernBERT-base
  load_backbone_weights: false
  config_args:
    # vocab_size: 1024  # will be set at bulid time (depending on the data vocabulary)
    max_position_embeddings: 512  # original value in ModernBERT-base: 8192
    output_hidden_states: true  # required for plotting embeddings
    classifier_pooling: mean  # to classify sequences during the fine-tuning phase
    pad_token_id: 0
    bos_token_id: 2  # 1 being mask_token_id
    cls_token_id: 2
    eos_token_id: 3
    sep_token_id: 3
    # Comment below for default ModernBERT config (so far provided better results)
    # Careful! Layer types should match num_hidden_layers and end with full_attention!
    # num_hidden_layers: 4      #   4 //    8 //   12
    # num_attention_heads: 4    #   4 //    8 //   12
    # hidden_size: 128          # 128 //  256 //  512
    # intermediate_size: 512    # 512 // 1024 // 2048
    # layer_types: [full_attention, sliding_attention, sliding_attention, full_attention]
  model_args:
    dtype: bfloat16
    attn_implementation: sdpa  # flash_attention_2 will create a shape mismatch in ModernBERT, with input_embeds as input
  embedding_layer_config:
    sentence_embedding_model: NeuML/pubmedbert-base-embeddings