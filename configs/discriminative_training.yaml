# Arguments for HF's trainer for the pre-training phase
pretraining:
  logging_steps: 10
  warmup_steps: 1000
  num_train_epochs: 100  # maximum number of epochs (loses to max_steps)
  max_steps: 100000  # maximum number of steps (training + validation)
  eval_strategy: steps
  save_strategy: steps
  eval_steps: 250
  save_steps: 250
  save_total_limit: 1
  load_best_model_at_end: true
  gradient_accumulation_steps: 1
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  task: masked  # will be popped out in the train_classification script
  learning_rate: 1.0e-4  # careful! "1e-4" will be read as a string
  weight_decay: 0.1
  bf16: true
  report_to: wandb
  remove_unused_columns: false  # crucial for custom collators
  load_best_model_at_end: true  # required for early stopping
  metric_for_best_model: eval_mlm_accuracy # format: "eval_" + metric name
  greater_is_better: true
  patience: 10  # will be popped out in the train_mlm script
  output_dir: results/pretraining
  logging_dir: results/pretraining/logs

# Arguments for HF's trainer for the fine-tuning phase
finetuning:
  logging_steps: 10
  warmup_steps: 1000
  num_train_epochs: 10  # maximum number of epochs (loses to max_steps)
  max_steps: 10000  # maximum number of steps (training + validation)
  eval_strategy: steps
  save_strategy: steps
  eval_steps: 250
  save_steps: 250
  save_total_limit: 1
  gradient_accumulation_steps: 1
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  learning_rate: 2.0e-4
  weight_decay: 0.1
  bf16: true
  report_to: wandb
  remove_unused_columns: false  # crucial for custom collators
  load_best_model_at_end: true
  metric_for_best_model: eval_roc_auc
  greater_is_better: true
  patience: 5  # will be popped out in the train_classification script
  output_dir: results/finetuning
  logging_dir: results/finetuning/logs

# Low-rank adaptation configuration for fine-tuning
peft:
  task_type: SEQ_CLS
  inference_mode: false
  r: 32
  lora_alpha: 64
  lora_dropout: 0.1
  target_modules: all-linear

# Arguments for my own evaluator class
eval_kwargs:
  do_clustering: false       # within evaluation loop, whether reduce and cluster embeddings
  max_clustered_samples: 2500  # maximum number of samples to include in the clustering analysis
  n_optuna_trials: 25    # within evaluation loop, None if you don't want to use optuna

# Embedding model
model:
  model_id: answerdotai/ModernBERT-base
  load_backbone_weights: false
  config_args:
    # vocab_size: 5036  # will be set at bulid time (depending on the data vocabulary)
    max_position_embeddings: 512  # original value in ModernBERT-base: 8192
    output_hidden_states: true  # required for plotting embeddings
    classifier_pooling: mean  # to classify sequences during the fine-tuning phase
    pad_token_id: 0
    bos_token_id: 2
    cls_token_id: 2
    eos_token_id: 3
    sep_token_id: 3
    # Comment what's below to get the default ModernBERT configuration
    # num_hidden_layers: 4      #   4 //    8 //   12
    # num_attention_heads: 4    #   4 //    8 //   12
    # hidden_size: 128          # 128 //  256 //  512
    # intermediate_size: 512    # 512 // 1024 // 2048
  model_args:
    dtype: bfloat16
    attn_implementation: sdpa  # flash_attention_2 will create a shape mismatch in ModernBERT, with input_embeds as input
  embedding_layer_config:
    vocab_mapping:
      entity: entity_id
      attribute: attribute_id
      value_binned: value_id
    time_key: days_since_tpx
    pretrained_model_name: NeuML/pubmedbert-base-embeddings

# Data collator configuration
data_collator:
  pad_token_id: 0
  mask_token_id: 1
  bos_token_id: 2
  eos_token_id: 3
  unk_token_id: 4
  mlm_probability: 0.15
  return_tensors: pt
  input_keys:
    - entity_id
    - attribute_id
  mlm_masked_key: value_id  # what is masked in the input
  mlm_label_key: value_id  # what is reconstructed in the mlm task
  time_key: days_since_tpx