# Run configuration and task (data collator) definition
hf_data_dir: /home/users/b/borneta/aiidkit_mhmmdrz/processed_data
result_dir: results  # base directory name for all results
data_collator:
  feature_keys:
    - days_since_tpx
    - entity_id
    - attribute_id
    - value_id
  time_key: days_since_tpx
  mlm_masking_rules:    # these numbers will set the result subdirectory name
    entity_id: 0.10     # mlm task masks 'entity_id' with this probability
    attribute_id: 0.20  # mlm task masks 'attribute_id' with this probability
    value_id: 0.40      # mlm task masks 'value_id' with this probability
  mlm_label_keys:
    - entity_id         # mlm task must predict 'entity_id' labels
    - attribute_id      # mlm task must predict 'attribute_id' labels
    - value_id          # mlm task must predict 'value_id' labels

# Configuration for finetuning tasks to run
prediction_tasks:
  infection_bacteria: {fups: [0, 30, 60, 90, 180, 365], horizons: [30, 60, 90]}
  # infection_virus:    {fups: [0, 30, 60, 90, 180, 365], horizons: [30, 60, 90]}
  # infection_fungi:    {fups: [0, 30, 60, 90, 180, 365], horizons: [30, 60, 90]}
  # infection:          {fups: [0, 30, 60, 90, 180, 365], horizons: [30, 60, 90]}
  graft_loss:         {fups: [0, 180, 365, 730, 1095],  horizons: [365, 730, 1095]}
  death:              {fups: [0, 180, 365, 730, 1095],  horizons: [365, 730, 1095]}

# Arguments for HF's trainer for the pre-training phase
pretrainer:
  logging_steps: 10
  warmup_steps: 1000
  num_train_epochs: 100  # maximum number of epochs (loses to max_steps)
  max_steps: 100000  # maximum number of steps (training + validation)
  eval_strategy: steps
  save_strategy: steps
  eval_steps: 250
  save_steps: 250
  save_total_limit: 1
  gradient_accumulation_steps: 1
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  learning_rate: 1.0e-4  # careful! "1e-4" will be read as a string, use 1.0e-4
  weight_decay: 0.1
  bf16: true
  report_to: wandb
  remove_unused_columns: false  # crucial for custom collators
  load_best_model_at_end: true  # required for early stopping
  metric_for_best_model: eval_mlm_accuracy  # format: "eval_" + metric name
  greater_is_better: true
  early_stopping_patience: 10  # evaluations without improvement before stopping

# Arguments for HF's trainer for the fine-tuning phase
finetuner:
  logging_steps: 10
  warmup_steps: 1000
  num_train_epochs: 10  # maximum number of epochs (loses to max_steps)
  max_steps: 10000  # maximum number of steps (training + validation)
  eval_strategy: steps
  save_strategy: steps
  eval_steps: 100
  save_steps: 100
  save_total_limit: 1
  gradient_accumulation_steps: 1
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  learning_rate: 5.0e-5
  weight_decay: 0.01
  bf16: true
  report_to: wandb
  remove_unused_columns: false  # crucial for custom collators
  load_best_model_at_end: true
  metric_for_best_model: eval_roc_auc  # eval_pr_auc
  greater_is_better: true
  early_stopping_patience: 20  # evaluations without improvement before stopping

# Embedding model
model:
  model_id: answerdotai/ModernBERT-base
  load_backbone_weights: false
  config_args:
    # vocab_size: 1024  # will be set at bulid time (depending on the data vocabulary)
    max_position_embeddings: 512  # original value in ModernBERT-base: 8192
    output_hidden_states: true  # required for plotting embeddings
    classifier_pooling: mean  # to classify sequences during the fine-tuning phase
    pad_token_id: 0
    bos_token_id: 2  # 1 being kept for mask_token_id
    cls_token_id: 2
    eos_token_id: 3
    sep_token_id: 3
    # Below should be commented to get the default ModernBERT configuration
    # num_hidden_layers: 4      #   4 //    8 //   12
    # num_attention_heads: 4    #   4 //    8 //   12
    # hidden_size: 128          # 128 //  256 //  512
    # intermediate_size: 512    # 512 // 1024 // 2048
  model_args:
    dtype: bfloat16
    attn_implementation: sdpa  # flash_attention_2 will create a shape mismatch in ModernBERT, with input_embeds as input
  embedding_layer_config:
    time_key: days_since_tpx
    sentence_embedding_model: NeuML/pubmedbert-base-embeddings