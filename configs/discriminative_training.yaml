# Run configuration and task (data collator) definition
hf_data_dir: /home/users/b/borneta/aiidkit_mhmmdrz/processed_data
result_dir: results  # base directory name for all results
data_collator:
  eav_mappings:
    entity_id: entity
    attribute_id: attribute
    value_id: value_binned
  time_mapping:
    days_since_tpx: time
  mlm_masking_rules:  # these numbers will set the result subdirectory name
    # {"entity_id": 0.00, "attribute_id": 0.00, "value_id": 0.15} NOT NOW
    # {"entity_id": 0.00, "attribute_id": 0.00, "value_id": 0.25}
    # {"entity_id": 0.00, "attribute_id": 0.00, "value_id": 0.45} NOT NOW
    # {"entity_id": 0.00, "attribute_id": 0.00, "value_id": 0.75} NOT NOW
    # {"entity_id": 0.00, "attribute_id": 0.05, "value_id": 0.15} NOT NOW
    # {"entity_id": 0.00, "attribute_id": 0.15, "value_id": 0.45} NOT NOW
    # {"entity_id": 0.00, "attribute_id": 0.25, "value_id": 0.75} NOT NOW
    # {"entity_id": 0.05, "attribute_id": 0.15, "value_id": 0.25}
    # {"entity_id": 0.15, "attribute_id": 0.25, "value_id": 0.45}
    # {"entity_id": 0.05, "attribute_id": 0.05, "value_id": 0.05}  NOW NOW
    # {"entity_id": 0.15, "attribute_id": 0.15, "value_id": 0.15}
    {"entity_id": 0.25, "attribute_id": 0.25, "value_id": 0.25}
  mlm_label_keys:
    - entity_id         # mlm task must predict 'entity_id' labels
    - attribute_id      # mlm task must predict 'attribute_id' labels
    - value_id          # mlm task must predict 'value_id' labels

# PRETRAINING A FULL MODERN BERT MODEL TAKES APPROXIMATELY 2 HOURS
# FINETUNING TAKES 30 MINUTES PER HORIZON, SO COUNT AROUND 2 HOURS PER TASK

# Configuration for finetuning tasks to run
prediction_tasks:
  infection_bacteria: {fups: [0, 30, 90], horizons: [30, 60, 90, 275, 335, 365]}
  # infection_bacteria: {fups: [0, 30, 60, 90, 180, 365], horizons: [30, 60, 90]}
  # infection_virus:    {fups: [0, 30, 60, 90, 180, 365], horizons: [30, 60, 90]}
  # infection_fungi:    {fups: [0, 30, 60, 90, 180, 365], horizons: [30, 60, 90]}
  # infection:          {fups: [0, 30, 60, 90, 180, 365], horizons: [30, 60, 90]}
  # graft_loss:         {fups: [0, 180, 365, 730, 1095],  horizons: [180, 365, 730, 1095]}
  # death:              {fups: [0, 180, 365, 730, 1095],  horizons: [180, 365, 730, 1095]}

# Arguments for HF's trainer for the pre-training phase
pretrainer:
  logging_steps: 10
  warmup_steps: 1000
  num_train_epochs: 100  # maximum number of epochs (loses to max_steps)
  max_steps: 100000  # maximum number of steps (training + validation)
  eval_strategy: steps
  save_strategy: steps
  eval_steps: 250
  save_steps: 250
  save_total_limit: 1
  gradient_accumulation_steps: 1
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  learning_rate: 1.0e-4  # careful! "1e-4" will be read as a string, use 1.0e-4
  weight_decay: 0.1
  bf16: true
  report_to: wandb
  remove_unused_columns: false  # crucial for custom collators
  load_best_model_at_end: true  # required for early stopping
  metric_for_best_model: eval_mlm_accuracy  # format: "eval_" + metric name
  greater_is_better: true
  early_stopping_patience: 10  # evaluations without improvement before stopping

# Arguments for HF's trainer for the fine-tuning phase
finetuner:
  logging_steps: 10
  warmup_steps: 1000
  num_train_epochs: 10  # maximum number of epochs (loses to max_steps)
  max_steps: 100000  # maximum number of steps (training + validation)
  eval_strategy: steps
  save_strategy: steps
  eval_steps: 100
  save_steps: 100
  save_total_limit: 1
  gradient_accumulation_steps: 1
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  learning_rate: 4.0e-5
  weight_decay: 0.01
  bf16: true
  report_to: wandb
  remove_unused_columns: false  # crucial for custom collators
  load_best_model_at_end: true
  metric_for_best_model: eval_all_roc_auc  # eval_all_pr_auc
  greater_is_better: true
  early_stopping_patience: 20  # evaluations without improvement before stopping

# Embedding model
model:
  model_id: answerdotai/ModernBERT-base
  load_backbone_weights: false
  config_args:
    # vocab_size: 1024  # will be set at bulid time (depending on the data vocabulary)
    max_position_embeddings: 512  # original value in ModernBERT-base: 8192
    output_hidden_states: true  # required for plotting embeddings
    classifier_pooling: mean  # to classify sequences during the fine-tuning phase
    pad_token_id: 0
    bos_token_id: 2  # 1 being mask_token_id
    cls_token_id: 2
    eos_token_id: 3
    sep_token_id: 3
    # Comment below for default ModernBERT config (so far provided better results)
    # num_hidden_layers: 4      #   4 //    8 //   12
    # num_attention_heads: 4    #   4 //    8 //   12
    # hidden_size: 128          # 128 //  256 //  512
    # intermediate_size: 512    # 512 // 1024 // 2048
  model_args:
    dtype: bfloat16
    attn_implementation: sdpa  # flash_attention_2 will create a shape mismatch in ModernBERT, with input_embeds as input
  embedding_layer_config:
    sentence_embedding_model: NeuML/pubmedbert-base-embeddings