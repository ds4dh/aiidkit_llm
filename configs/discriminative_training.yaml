# Arguments for HF's trainer for the pre-training phase
pretrainer:
  logging_steps: 10
  warmup_steps: 1000
  num_train_epochs: 100  # maximum number of epochs (loses to max_steps)
  max_steps: 100000  # maximum number of steps (training + validation)
  eval_strategy: steps
  save_strategy: steps
  eval_steps: 250
  save_steps: 250
  save_total_limit: 1
  gradient_accumulation_steps: 1
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  learning_rate: 1.0e-4  # careful! "1e-4" will be read as a string
  weight_decay: 0.1
  bf16: true
  report_to: wandb
  remove_unused_columns: false  # crucial for custom collators
  load_best_model_at_end: true  # required for early stopping
  metric_for_best_model: eval_mlm_accuracy # format: "eval_" + metric name
  greater_is_better: true
  output_dir: results/pretraining
  logging_dir: results/pretraining/logs

# Arguments for HF's trainer for the fine-tuning phase
finetuner:
  logging_steps: 10
  warmup_steps: 1000
  num_train_epochs: 10  # maximum number of epochs (loses to max_steps)
  max_steps: 10000  # maximum number of steps (training + validation)
  eval_strategy: steps
  save_strategy: steps
  eval_steps: 100
  save_steps: 100
  save_total_limit: 1
  gradient_accumulation_steps: 1
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  learning_rate: 1.0e-5
  weight_decay: 0.01
  bf16: true
  report_to: wandb
  remove_unused_columns: false  # crucial for custom collators
  load_best_model_at_end: true
  metric_for_best_model: eval_roc_auc  # eval_pr_auc
  greater_is_better: true
  output_dir: results/finetuning
  logging_dir: results/finetuning/logs

# Embedding model
model:
  model_id: answerdotai/ModernBERT-base
  load_backbone_weights: false
  config_args:
    # vocab_size: 5036  # will be set at bulid time (depending on the data vocabulary)
    max_position_embeddings: 512  # original value in ModernBERT-base: 8192
    output_hidden_states: true  # required for plotting embeddings
    classifier_pooling: mean  # to classify sequences during the fine-tuning phase
    pad_token_id: 0
    bos_token_id: 2
    cls_token_id: 2
    eos_token_id: 3
    sep_token_id: 3
    # Comment what's below to get the default ModernBERT configuration
    # num_hidden_layers: 4      #   4 //    8 //   12
    # num_attention_heads: 4    #   4 //    8 //   12
    # hidden_size: 128          # 128 //  256 //  512
    # intermediate_size: 512    # 512 // 1024 // 2048
  model_args:
    dtype: bfloat16
    attn_implementation: sdpa  # flash_attention_2 will create a shape mismatch in ModernBERT, with input_embeds as input
  embedding_layer_config:
    time_key: days_since_tpx
    pretrained_model_name: NeuML/pubmedbert-base-embeddings

# Data collator configuration
data_collator:
  pad_token_id: 0
  mask_token_id: 1
  bos_token_id: 2
  eos_token_id: 3
  unk_token_id: 4
  return_tensors: pt
  input_keys:
    - entity_id
    - attribute_id
  time_key: days_since_tpx
  mlm_masking_rules:
    value_id: 0.5       # mask 'value_id' with 50% probability
    entity_id: 0.5      # mask 'entity_id' with 50% probability
  mlm_label_keys:
    - value_id          # generate labels for 'value_id'
    - entity_id         # generate labels for 'entity_id'